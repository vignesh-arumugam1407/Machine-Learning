{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc72a70a",
   "metadata": {},
   "source": [
    "What is Supervised Learning (Classification)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50191cdc",
   "metadata": {},
   "source": [
    "Supervised Learning is a type of machine learning where the model learns from labeled data. In classification, the goal is to predict a category or class label (e.g., spam/not spam, yes/no, disease/no disease)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c20d16",
   "metadata": {},
   "source": [
    "| Algorithm                                          | Type                 | Use Cases                                   | Example                   |\n",
    "| -------------------------------------------------- | -------------------- | ------------------------------------------- | ------------------------- |\n",
    "| **1. Logistic Regression**                         | Linear               | Binary classification, medical diagnosis    | Spam detection (spam/ham) |\n",
    "| **2. K-Nearest Neighbors (KNN)**                   | Lazy, non-parametric | Recommendation systems, pattern recognition | Classify image as dog/cat |\n",
    "| **3. Support Vector Machine (SVM)**                | Linear/non-linear    | Text classification, face detection         | Sentiment analysis        |\n",
    "| **4. Decision Tree**                               | Non-linear           | Rule-based systems, fraud detection         | Loan approval             |\n",
    "| **5. Random Forest**                               | Ensemble             | Credit scoring, feature importance          | Classify loan defaulters  |\n",
    "| **6. Naive Bayes**                                 | Probabilistic        | Text classification, spam filtering         | Email spam classification |\n",
    "| **7. Gradient Boosting** (XGBoost, LightGBM, etc.) | Ensemble             | High-performance tasks                      | Disease prediction        |\n",
    "| **8. Neural Networks (MLP)**                       | Deep learning        | Image & speech classification               | Handwriting recognition   |\n",
    "| **9. Histogram-based Gradient Boosting (HGB)**     | Scalable ensemble    | Large datasets                              | Click prediction          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd31be15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.958041958041958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\AI & ML\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "'''\n",
    "Use: Binary classification\n",
    "\n",
    "How it works: Uses sigmoid function to output probabilities between 0 and 1.'''\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression     #Model used for binary classification.\n",
    "from sklearn.datasets import load_breast_cancer         #Loads the breast cancer dataset.\n",
    "from sklearn.model_selection import train_test_split    #Splits the dataset into training and testing parts.\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)              # Feature matrix X and target vector y.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.25,random_state=42)   # Splits data (default: 75% train, 25% test).\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)               #initializes the model, allowing up to 1000 iterations for convergence.\n",
    "model.fit(X_train, y_train)                             #trains the model using the training data.\n",
    "print(model.score(X_test, y_test))                      #calculates accuracy — that is, how well your model performs on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc5ae34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9020979020979021\n"
     ]
    }
   ],
   "source": [
    "#K-Nearest Neighbors (KNN)\n",
    "'''\n",
    "Use: Pattern recognition\n",
    "\n",
    "How it works: Classifies a data point based on the majority label of its k-nearest neighbors.\n",
    "'''\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=3)  # The model to look at the 3 nearest neighbors when making predictions.- It will classify a test point based on the majority vote among these 3 neighbors.\n",
    "\n",
    "model.fit(X_train, y_train)         #- Stores the training data. KNN is a lazy learner, so it doesn’t build a model upfront — it just memorizes the data and computes distances at prediction time.\n",
    "\n",
    "print(model.score(X_test, y_test))     # Returns the accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25008d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.951048951048951\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine (SVM)\n",
    "'''\n",
    "Use: Text, image classification\n",
    "\n",
    "How it works: Finds the hyperplane that best separates classes.\n",
    "'''\n",
    "\n",
    "from sklearn.svm import SVC         #Stands for Support Vector Classification, which is a powerful algorithm for both linear and non-linear classification tasks.\n",
    "\n",
    "model = SVC(kernel='linear')        #SVM model using a linear kernel, which means it will attempt to find a linear hyperplane that separates the data.\n",
    "model.fit(X_train, y_train)         #Fits the SVM model to the training data.\n",
    "print(model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b66d96",
   "metadata": {},
   "source": [
    "| Kernel      | Description                                                                    | Use Case                                 |\n",
    "| ----------- | ------------------------------------------------------------------------------ | ---------------------------------------- |\n",
    "| `'linear'`  | No transformation. Tries to separate data with a **straight line (or plane)**. | When data is linearly separable.         |\n",
    "| `'poly'`    | Polynomial kernel. Allows **curved decision boundaries**.                      | When the relationship is polynomial.     |\n",
    "| `'rbf'`     | Radial Basis Function (Gaussian). Most commonly used.                          | For non-linear, complex data structures. |\n",
    "| `'sigmoid'` | Uses the sigmoid function. Similar to a neural network activation function.    | Rarely used.                             |\n",
    "| `custom`    | You can define your own kernel function.                                       | For very specific use cases.             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbae8226",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision tree\n",
    "'''\n",
    "Use: Rule-based systems\n",
    "\n",
    "How it works: Creates a tree where each node is a decision rule.\n",
    "'''\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "'''\n",
    "Initializes a decision tree classifier using default parameters:\n",
    "\n",
    "criterion='gini' (default): uses the Gini impurity to split nodes.\n",
    "\n",
    "max_depth=None: tree expands until all leaves are pure or contain less than min_samples_split.\n",
    "\n",
    "random_state=None: randomness in tree construction is not fixed.\n",
    "'''\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a57a1f",
   "metadata": {},
   "source": [
    "DECISION TREE       ||    COMMAN ISSUES AND CONSIDERATION "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac05a61",
   "metadata": {},
   "source": [
    "| Problem                   | Cause                           | Fix                                        |\n",
    "| ------------------------- | ------------------------------- | ------------------------------------------ |\n",
    "| **Overfitting**           | Tree is too deep and fits noise | Use `max_depth`, `min_samples_split`, etc. |\n",
    "| **Poor generalization**   | Not enough data pruning         | Prune or restrict tree growth              |\n",
    "| **Inconsistent accuracy** | Randomness in tree construction | Set `random_state`                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c613ef12",
   "metadata": {},
   "source": [
    "You can control overfitting by tuning hyperparameters that prune or restrict the tree:\n",
    "\n",
    "| Parameter           | Description                                                   | Helps Fix Overfitting? |\n",
    "| ------------------- | ------------------------------------------------------------- | ---------------------- |\n",
    "| `max_depth`         | The maximum depth of the tree.                                | ✅ Yes                  |\n",
    "| `min_samples_split` | Minimum samples required to split an internal node.           | ✅ Yes                  |\n",
    "| `min_samples_leaf`  | Minimum samples required at a leaf node.                      | ✅ Yes                  |\n",
    "| `max_leaf_nodes`    | Limit on number of leaf nodes.                                | ✅ Yes                  |\n",
    "| `max_features`      | Number of features to consider when splitting.                | ✅ Yes                  |\n",
    "| `ccp_alpha`         | Complexity parameter for **post-pruning**.                    | ✅ Yes                  |\n",
    "| `random_state`      | Seed for randomness (for reproducibility).                    | ❌ No                   |\n",
    "| `criterion`         | Function to measure quality of a split (`gini` or `entropy`). | ❌ No                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a103685f",
   "metadata": {},
   "source": [
    "All Key Parameters in DecisionTreeClassifier (with Explanation)\n",
    "| Parameter                  | Description                                                                               |\n",
    "| -------------------------- | ----------------------------------------------------------------------------------------- |\n",
    "| `criterion`                | `'gini'` (default) or `'entropy'`. Measure used to select best splits.                    |\n",
    "| `splitter`                 | `'best'` (default) or `'random'`. Strategy used to choose the split at each node.         |\n",
    "| `max_depth`                | Maximum depth of the tree. Prevents the tree from growing too deep.                       |\n",
    "| `min_samples_split`        | Minimum number of samples required to split a node. Default is 2.                         |\n",
    "| `min_samples_leaf`         | Minimum number of samples at a leaf node. Prevents leaves with very few samples.          |\n",
    "| `max_features`             | Max number of features to consider when looking for the best split.                       |\n",
    "| `max_leaf_nodes`           | Grow tree with at most this many leaf nodes.                                              |\n",
    "| `min_weight_fraction_leaf` | Like `min_samples_leaf` but uses fraction of total weights (for weighted samples).        |\n",
    "| `max_samples`              | (Since v1.1) Only available if `bootstrap=True`. Used to subsample the data.              |\n",
    "| `random_state`             | Controls randomness in tree building (e.g. `splitter='random'`). Ensures reproducibility. |\n",
    "| `ccp_alpha`                | Complexity parameter used for **Minimal Cost-Complexity Pruning**. Higher → simpler tree. |\n",
    "| `class_weight`             | Can be used to handle imbalanced classes.                                                 |\n",
    "| `presort`                  | Deprecated. Now always presorts data for faster split finding.                            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fef7ce",
   "metadata": {},
   "source": [
    " What is a Random Forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabfebda",
   "metadata": {},
   "source": [
    "A Random Forest is an ensemble of multiple decision trees, where:\n",
    "\n",
    "Each tree is trained on a random subset of the data and features.\n",
    "\n",
    "Final prediction is made by voting (classification) or averaging (regression).\n",
    "\n",
    "It's robust, less prone to overfitting, and works well even without feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fea35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9790209790209791\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "\n",
    "''' \n",
    "Use: Robust predictions\n",
    "\n",
    "How it works: Combines multiple decision trees (ensemble).\n",
    "'''\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "''' \n",
    "Uses 100 trees (n_estimators=100) by default.\n",
    "\n",
    "Each tree is built on a random sample with replacement (bootstrap).\n",
    "\n",
    "Splits are made using a random subset of features at each node.\n",
    "'''\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e46032",
   "metadata": {},
   "source": [
    "Why use Random Forest?\n",
    "| Feature                   | Benefit                                             |\n",
    "| ------------------------- | --------------------------------------------------- |\n",
    "| **Reduces overfitting**   | Averaging over many trees smooths out the variance. |\n",
    "| **Handles non-linearity** | Can capture complex patterns.                       |\n",
    "| **Works without scaling** | No need to normalize features.                      |\n",
    "| **Feature importance**    | You can find which features are most useful.        |\n",
    "| **Robust**                | Works well even with missing values or noisy data.  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cc5b7e",
   "metadata": {},
   "source": [
    "Important Parameters\n",
    "| Parameter           | Description                                                            | Helps Control          |\n",
    "| ------------------- | ---------------------------------------------------------------------- | ---------------------- |\n",
    "| `n_estimators`      | Number of trees in the forest. More trees → better results but slower. | Bias-variance tradeoff |\n",
    "| `criterion`         | `'gini'` (default) or `'entropy'`. Split quality metric.               | Accuracy               |\n",
    "| `max_depth`         | Max depth of trees. Prevents overfitting.                              | Overfitting            |\n",
    "| `min_samples_split` | Minimum samples to split an internal node.                             | Overfitting            |\n",
    "| `min_samples_leaf`  | Minimum samples in a leaf node.                                        | Overfitting            |\n",
    "| `max_features`      | Number of features to consider when splitting.                         | Performance            |\n",
    "| `bootstrap`         | Whether sampling is done with replacement.                             | Diversity              |\n",
    "| `random_state`      | Reproducibility of results.                                            | Consistency            |\n",
    "| `n_jobs=-1`         | Use all CPU cores for faster training.                                 | Speed                  |\n",
    "| `oob_score=True`    | Use out-of-bag samples for validation.                                 | Model evaluation       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7e3bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#Tuning the MODEL\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    min_samples_leaf=4,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    oob_score=True\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09440a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Feature Importance (After Training)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "feature_importances = pd.Series(model.feature_importances_, index=feature_names)\n",
    "feature_importances.sort_values(ascending=False).head()\n",
    "\n",
    "#Helps understand which features are most influential in predictions.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aee1087",
   "metadata": {},
   "source": [
    "🧠 What Is Naive Bayes?\n",
    "Naive Bayes is a probabilistic classifier based on Bayes’ Theorem with the naive assumption that all features are independent of each other.\n",
    "\n",
    "📊 Gaussian Naive Bayes\n",
    "The GaussianNB classifier assumes that features follow a normal (Gaussian) distribution.\n",
    "\n",
    "Best suited for continuous input features (like those in the Breast Cancer dataset).\n",
    "\n",
    "It’s fast, simple, and often performs well for high-dimensional datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dc994d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.951048951048951\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes\n",
    "''' \n",
    "Use: Spam filters\n",
    "\n",
    "How it works: Uses Bayes theorem assuming feature independence.\n",
    "'''\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()            #Initializes the model with default parameters.Assumes each feature is normally distributed within each class.\n",
    "model.fit(X_train, y_train)     #Trains the model using the training data.It estimates the mean and variance of each feature for each class.\n",
    "print(model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "016809f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-3.0.2-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in d:\\python\\ai & ml\\.venv\\lib\\site-packages (from xgboost) (2.3.1)\n",
      "Requirement already satisfied: scipy in d:\\python\\ai & ml\\.venv\\lib\\site-packages (from xgboost) (1.16.0)\n",
      "Downloading xgboost-3.0.2-py3-none-win_amd64.whl (150.0 MB)\n",
      "   ---------------------------------------- 0.0/150.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/150.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/150.0 MB 2.5 MB/s eta 0:01:01\n",
      "   ---------------------------------------- 1.0/150.0 MB 2.5 MB/s eta 0:01:01\n",
      "   ---------------------------------------- 1.6/150.0 MB 2.5 MB/s eta 0:00:59\n",
      "    --------------------------------------- 2.6/150.0 MB 3.3 MB/s eta 0:00:45\n",
      "    --------------------------------------- 3.7/150.0 MB 3.7 MB/s eta 0:00:40\n",
      "   - -------------------------------------- 5.2/150.0 MB 4.3 MB/s eta 0:00:34\n",
      "   - -------------------------------------- 6.6/150.0 MB 4.7 MB/s eta 0:00:31\n",
      "   -- ------------------------------------- 7.9/150.0 MB 4.9 MB/s eta 0:00:30\n",
      "   -- ------------------------------------- 8.9/150.0 MB 4.9 MB/s eta 0:00:29\n",
      "   -- ------------------------------------- 9.7/150.0 MB 4.9 MB/s eta 0:00:29\n",
      "   -- ------------------------------------- 11.0/150.0 MB 4.8 MB/s eta 0:00:29\n",
      "   --- ------------------------------------ 12.1/150.0 MB 4.9 MB/s eta 0:00:29\n",
      "   --- ------------------------------------ 13.4/150.0 MB 5.0 MB/s eta 0:00:28\n",
      "   --- ------------------------------------ 14.7/150.0 MB 5.1 MB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 15.5/150.0 MB 5.1 MB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 16.3/150.0 MB 4.9 MB/s eta 0:00:28\n",
      "   ---- ----------------------------------- 17.3/150.0 MB 4.9 MB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 18.9/150.0 MB 5.0 MB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 20.2/150.0 MB 5.1 MB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 21.5/150.0 MB 5.2 MB/s eta 0:00:25\n",
      "   ------ --------------------------------- 22.8/150.0 MB 5.2 MB/s eta 0:00:25\n",
      "   ------ --------------------------------- 24.1/150.0 MB 5.2 MB/s eta 0:00:25\n",
      "   ------ --------------------------------- 25.4/150.0 MB 5.3 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 26.5/150.0 MB 5.3 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 27.5/150.0 MB 5.3 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 28.6/150.0 MB 5.3 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 29.4/150.0 MB 5.2 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 30.1/150.0 MB 5.1 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 30.9/150.0 MB 5.1 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 31.7/150.0 MB 5.0 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 32.8/150.0 MB 5.0 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 33.3/150.0 MB 5.0 MB/s eta 0:00:24\n",
      "   --------- ------------------------------ 34.1/150.0 MB 4.9 MB/s eta 0:00:24\n",
      "   --------- ------------------------------ 34.9/150.0 MB 4.9 MB/s eta 0:00:24\n",
      "   --------- ------------------------------ 36.2/150.0 MB 4.9 MB/s eta 0:00:24\n",
      "   --------- ------------------------------ 37.2/150.0 MB 4.9 MB/s eta 0:00:24\n",
      "   ---------- ----------------------------- 38.0/150.0 MB 4.9 MB/s eta 0:00:23\n",
      "   ---------- ----------------------------- 39.1/150.0 MB 4.9 MB/s eta 0:00:23\n",
      "   ---------- ----------------------------- 39.8/150.0 MB 4.9 MB/s eta 0:00:23\n",
      "   ---------- ----------------------------- 40.6/150.0 MB 4.8 MB/s eta 0:00:23\n",
      "   ----------- ---------------------------- 41.4/150.0 MB 4.8 MB/s eta 0:00:23\n",
      "   ----------- ---------------------------- 42.2/150.0 MB 4.8 MB/s eta 0:00:23\n",
      "   ----------- ---------------------------- 43.0/150.0 MB 4.8 MB/s eta 0:00:23\n",
      "   ----------- ---------------------------- 44.0/150.0 MB 4.7 MB/s eta 0:00:23\n",
      "   ----------- ---------------------------- 44.8/150.0 MB 4.7 MB/s eta 0:00:23\n",
      "   ------------ --------------------------- 45.6/150.0 MB 4.7 MB/s eta 0:00:23\n",
      "   ------------ --------------------------- 46.4/150.0 MB 4.7 MB/s eta 0:00:23\n",
      "   ------------ --------------------------- 47.4/150.0 MB 4.7 MB/s eta 0:00:22\n",
      "   ------------ --------------------------- 48.5/150.0 MB 4.7 MB/s eta 0:00:22\n",
      "   ------------- -------------------------- 49.5/150.0 MB 4.7 MB/s eta 0:00:22\n",
      "   ------------- -------------------------- 50.1/150.0 MB 4.7 MB/s eta 0:00:22\n",
      "   ------------- -------------------------- 50.9/150.0 MB 4.7 MB/s eta 0:00:22\n",
      "   ------------- -------------------------- 51.6/150.0 MB 4.6 MB/s eta 0:00:22\n",
      "   ------------- -------------------------- 52.4/150.0 MB 4.6 MB/s eta 0:00:22\n",
      "   -------------- ------------------------- 53.2/150.0 MB 4.6 MB/s eta 0:00:22\n",
      "   -------------- ------------------------- 54.0/150.0 MB 4.6 MB/s eta 0:00:21\n",
      "   -------------- ------------------------- 54.5/150.0 MB 4.6 MB/s eta 0:00:21\n",
      "   -------------- ------------------------- 55.1/150.0 MB 4.5 MB/s eta 0:00:21\n",
      "   -------------- ------------------------- 55.8/150.0 MB 4.5 MB/s eta 0:00:21\n",
      "   --------------- ------------------------ 56.6/150.0 MB 4.5 MB/s eta 0:00:21\n",
      "   --------------- ------------------------ 56.9/150.0 MB 4.5 MB/s eta 0:00:21\n",
      "   --------------- ------------------------ 57.1/150.0 MB 4.4 MB/s eta 0:00:22\n",
      "   --------------- ------------------------ 57.4/150.0 MB 4.4 MB/s eta 0:00:22\n",
      "   --------------- ------------------------ 57.7/150.0 MB 4.3 MB/s eta 0:00:22\n",
      "   --------------- ------------------------ 57.9/150.0 MB 4.2 MB/s eta 0:00:22\n",
      "   --------------- ------------------------ 57.9/150.0 MB 4.2 MB/s eta 0:00:22\n",
      "   --------------- ------------------------ 57.9/150.0 MB 4.2 MB/s eta 0:00:22\n",
      "   --------------- ------------------------ 57.9/150.0 MB 4.2 MB/s eta 0:00:22\n",
      "   --------------- ------------------------ 57.9/150.0 MB 4.2 MB/s eta 0:00:22\n",
      "   --------------- ------------------------ 57.9/150.0 MB 4.2 MB/s eta 0:00:22\n",
      "   --------------- ------------------------ 57.9/150.0 MB 4.2 MB/s eta 0:00:22\n",
      "   --------------- ------------------------ 57.9/150.0 MB 4.2 MB/s eta 0:00:22\n",
      "   --------------- ------------------------ 57.9/150.0 MB 4.2 MB/s eta 0:00:22\n",
      "   --------------- ------------------------ 57.9/150.0 MB 4.2 MB/s eta 0:00:22\n",
      "   --------------- ------------------------ 57.9/150.0 MB 4.2 MB/s eta 0:00:22\n",
      "   --------------- ------------------------ 57.9/150.0 MB 4.2 MB/s eta 0:00:22\n",
      "   --------------- ------------------------ 57.9/150.0 MB 4.2 MB/s eta 0:00:22\n",
      "   --------------- ------------------------ 57.9/150.0 MB 4.2 MB/s eta 0:00:22\n",
      "   --------------- ------------------------ 57.9/150.0 MB 4.2 MB/s eta 0:00:22\n",
      "   --------------- ------------------------ 57.9/150.0 MB 4.2 MB/s eta 0:00:22\n",
      "   --------------- ------------------------ 57.9/150.0 MB 4.2 MB/s eta 0:00:22\n",
      "   --------------- ------------------------ 57.9/150.0 MB 4.2 MB/s eta 0:00:22\n",
      "   --------------- ------------------------ 57.9/150.0 MB 4.2 MB/s eta 0:00:22\n",
      "   --------------- ------------------------ 58.2/150.0 MB 3.3 MB/s eta 0:00:28\n",
      "   --------------- ------------------------ 58.5/150.0 MB 3.3 MB/s eta 0:00:28\n",
      "   --------------- ------------------------ 59.0/150.0 MB 3.3 MB/s eta 0:00:28\n",
      "   --------------- ------------------------ 59.2/150.0 MB 3.2 MB/s eta 0:00:29\n",
      "   --------------- ------------------------ 59.8/150.0 MB 3.2 MB/s eta 0:00:28\n",
      "   ---------------- ----------------------- 60.6/150.0 MB 3.2 MB/s eta 0:00:28\n",
      "   ---------------- ----------------------- 61.3/150.0 MB 3.2 MB/s eta 0:00:28\n",
      "   ---------------- ----------------------- 62.1/150.0 MB 3.2 MB/s eta 0:00:28\n",
      "   ---------------- ----------------------- 63.7/150.0 MB 3.3 MB/s eta 0:00:27\n",
      "   ----------------- ---------------------- 65.3/150.0 MB 3.3 MB/s eta 0:00:26\n",
      "   ----------------- ---------------------- 66.6/150.0 MB 3.4 MB/s eta 0:00:25\n",
      "   ------------------ --------------------- 68.7/150.0 MB 3.4 MB/s eta 0:00:24\n",
      "   ------------------ --------------------- 70.8/150.0 MB 3.5 MB/s eta 0:00:23\n",
      "   ------------------- -------------------- 72.9/150.0 MB 3.6 MB/s eta 0:00:22\n",
      "   -------------------- ------------------- 75.2/150.0 MB 3.6 MB/s eta 0:00:21\n",
      "   --------------------- ------------------ 78.9/150.0 MB 3.8 MB/s eta 0:00:19\n",
      "   --------------------- ------------------ 82.1/150.0 MB 3.9 MB/s eta 0:00:18\n",
      "   ---------------------- ----------------- 85.2/150.0 MB 4.0 MB/s eta 0:00:17\n",
      "   ----------------------- ---------------- 89.4/150.0 MB 4.2 MB/s eta 0:00:15\n",
      "   ------------------------ --------------- 92.5/150.0 MB 4.3 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 97.0/150.0 MB 4.4 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 102.5/150.0 MB 4.6 MB/s eta 0:00:11\n",
      "   ---------------------------- ----------- 106.7/150.0 MB 4.8 MB/s eta 0:00:10\n",
      "   ----------------------------- ---------- 111.1/150.0 MB 4.9 MB/s eta 0:00:08\n",
      "   ------------------------------ --------- 115.1/150.0 MB 5.1 MB/s eta 0:00:07\n",
      "   -------------------------------- ------- 122.2/150.0 MB 5.3 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 129.2/150.0 MB 5.6 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 137.1/150.0 MB 5.9 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 143.7/150.0 MB 6.1 MB/s eta 0:00:02\n",
      "   ---------------------------------------  149.9/150.0 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 150.0/150.0 MB 6.3 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-3.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab6eee1",
   "metadata": {},
   "source": [
    "What Is XGBoost? \n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) is a fast, regularized, and scalable implementation of gradient boosting. It builds an ensemble of decision trees sequentially, where each tree tries to correct the errors of the previous ones.\n",
    "\n",
    "It’s known for:\n",
    "\n",
    "High performance\n",
    "\n",
    "Speed\n",
    "\n",
    "Built-in regularization to reduce overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d5f0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.993006993006993\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting\n",
    "''' \n",
    "Use: Structured data competitions\n",
    "\n",
    "How it works: Builds models sequentially to correct errors from previous models.\n",
    "'''\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "model = XGBClassifier()\n",
    "\n",
    "''' \n",
    "Initializes the classifier with default settings, including:\n",
    "\n",
    "n_estimators=100: number of boosting rounds (trees)\n",
    "\n",
    "learning_rate=0.3: how much each tree contributes\n",
    "\n",
    "max_depth=6: max depth of each tree\n",
    "\n",
    "use_label_encoder=False (for newer versions)\n",
    "\n",
    "eval_metric='logloss' (for binary classification)\n",
    "'''\n",
    "model.fit(X_train, y_train)     #Builds multiple trees sequentially using gradient descent to minimize the loss function.\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3ecd53",
   "metadata": {},
   "source": [
    " Key Advantages of XGBoost\n",
    " | Feature                                        | Benefit                                              |\n",
    "| ---------------------------------------------- | ---------------------------------------------------- |\n",
    "| **Regularization** (`reg_alpha`, `reg_lambda`) | Reduces overfitting                                  |\n",
    "| **Handling of missing values**                 | Automatically handled by the algorithm               |\n",
    "| **Tree pruning**                               | Uses *max depth* and *max leaves* for better control |\n",
    "| **Parallel training**                          | Faster than traditional GBM                          |\n",
    "| **Early stopping support**                     | Can stop training when no improvement is seen        |\n",
    "| **Custom loss functions**                      | Flexible for advanced use cases                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c96208f",
   "metadata": {},
   "source": [
    "Important Parameters (Commonly Tuned)\n",
    "| Parameter                  | Description                                                |\n",
    "| -------------------------- | ---------------------------------------------------------- |\n",
    "| `n_estimators`             | Number of boosting rounds (trees)                          |\n",
    "| `max_depth`                | Maximum depth of trees                                     |\n",
    "| `learning_rate` (or `eta`) | Shrinks the contribution of each tree                      |\n",
    "| `subsample`                | Percentage of rows used per tree                           |\n",
    "| `colsample_bytree`         | Percentage of features used per tree                       |\n",
    "| `gamma`                    | Minimum loss reduction to make a split                     |\n",
    "| `reg_alpha`                | L1 regularization                                          |\n",
    "| `reg_lambda`               | L2 regularization                                          |\n",
    "| `scale_pos_weight`         | Used for imbalanced data                                   |\n",
    "| `objective`                | Specifies the learning task (default: `'binary:logistic'`) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95756fb9",
   "metadata": {},
   "source": [
    "XGBoost Use Cases\n",
    "| Domain       | Use Case                                        |\n",
    "| ------------ | ----------------------------------------------- |\n",
    "| Finance      | Fraud detection, credit scoring                 |\n",
    "| Healthcare   | Disease prediction, risk classification         |\n",
    "| Marketing    | Churn prediction, customer segmentation         |\n",
    "| Competitions | Widely used in **Kaggle** and **AI challenges** |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6285cc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#Example with Tuned Parameters\n",
    "\n",
    "model = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2fc19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8881118881118881\n"
     ]
    }
   ],
   "source": [
    "#Multilayer Perceptron (Neural Network) A type of artificial neural network.\n",
    "''' \n",
    "Use: Deep learning for non-linear data and supervised learning tasks—especially classification\n",
    "\n",
    "How it works: Layers of neurons connected with weights.\n",
    "'''\n",
    "from sklearn.neural_network import MLPClassifier        #- MLPClassifier is a feedforward artificial neural network used for classification.\n",
    "\n",
    "model = MLPClassifier(max_iter=1000)\n",
    "model.fit(X_train, y_train)     #- Internally, it uses backpropagation and gradient descent to adjust weights and minimize classification error.\n",
    "print(model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512d3942",
   "metadata": {},
   "source": [
    "Use Cases :- (MLP Classifiier)\n",
    "\n",
    "| 📧 Spam detection | Classifies emails as spam or not | \n",
    "| 🖼️ Image classification | Distinguishes digits, animals, etc. | \n",
    "| 🏥 Medical diagnosis | Predicts conditions from patient data | \n",
    "| 🎓 Student performance | Predicts pass/fail based on study patterns | \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1113a9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.993006993006993\n"
     ]
    }
   ],
   "source": [
    "#Histogram-based Gradient Boosting\n",
    "''' \n",
    "Use: Scalable boosting for big data;  It’s faster and more memory-efficient than traditional Gradient Boosting, especially for large datasets.\n",
    "\n",
    "How it works: Faster implementation of gradient boosting\n",
    "'''\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier #- It uses gradient boosting with decision trees and histogram-based binning to speed up computation.\n",
    "\n",
    "model = HistGradientBoostingClassifier()    #- It automatically handles categorical features, missing values, and has built-in regularization.\n",
    "model.fit(X_train, y_train)     #- Internally, it builds an ensemble of decision trees, where each tree corrects the mistakes of the previous one.\n",
    "print(model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a03287",
   "metadata": {},
   "source": [
    "| Algorithm           | Best For                | Pros                 | Cons                         |\n",
    "| ------------------- | ----------------------- | -------------------- | ---------------------------- |\n",
    "| Logistic Regression | Simple binary problems  | Fast, interpretable  | Limited to linear boundaries |\n",
    "| KNN                 | Low-dimensional data    | Easy to implement    | Slow on large data           |\n",
    "| SVM                 | High-dimensional data   | Effective, flexible  | Needs tuning                 |\n",
    "| Decision Tree       | Rule-based logic        | Easy to understand   | Overfitting                  |\n",
    "| Random Forest       | Ensemble learning       | Accurate             | Less interpretable           |\n",
    "| Naive Bayes         | Text data               | Fast, scalable       | Assumes independence         |\n",
    "| Gradient Boosting   | Performance tasks       | High accuracy        | Training time                |\n",
    "| MLP/Neural Net      | Complex non-linear data | Deep learning ready  | Needs large data             |\n",
    "| HGB                 | Scalable gradient boost | Very fast & accurate | Newer model                  |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
