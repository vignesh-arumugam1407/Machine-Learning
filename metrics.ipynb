{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae8f7b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\AI & ML\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1406: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "data={\n",
    "    'Feature1':[1,2,3,4,5,6,7,8,9,10],\n",
    "    'Feature2':[10,9,8,7,6,5,4,3,2,1],\n",
    "    'Target':[1,0,0,1,0,1,1,0,0,1]\n",
    "}\n",
    "\n",
    "df=pd.DataFrame(data)\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "x=df[['Feature1','Feature2']]\n",
    "y=df[['Target']]\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25,random_state=42)\n",
    "model.fit(x_train,y_train)\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29247a31",
   "metadata": {},
   "source": [
    "EVALUATION or PERFORMANCE METRICS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3517c621",
   "metadata": {},
   "source": [
    "These performance metrics help us\n",
    "understand how well our model has\n",
    "performed for the given data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56572847",
   "metadata": {},
   "source": [
    "CLASSIFICATION MATRICS\n",
    "\n",
    "Accuracy    ||  Recall  ||  Precision   ||  F1 Score    ||  Confusion Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c1091d",
   "metadata": {},
   "source": [
    "ACCURACY\n",
    "\n",
    "Accuracy = (Number of Correct Predictions)/(Total Number of Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e2ef5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bbd81f",
   "metadata": {},
   "source": [
    "IMPORTANT TERMS\n",
    "\n",
    "True Positive (TP) is an outcome where the model correctly predicts the positive class.\n",
    "\n",
    "True Negative (TN) is an outcome where the model correctly\n",
    "predicts the negative class.\n",
    "\n",
    "False Positive (FP) is an outcome where the model incorrectly\n",
    "predicts the positive class.\n",
    "\n",
    "False Negative (EN) is an outcome where the model incorrectly\n",
    "predicts the negative class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e10154",
   "metadata": {},
   "source": [
    "PRECISON AND RECALL\n",
    "\n",
    "Precision is a measure of how many of the positive\n",
    "predictions made by a classification model were\n",
    "actually correct.\n",
    "\n",
    "Recall is a measure of how many of the actual\n",
    "positive instances win the dataset were correctly\n",
    "predicted by the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68822e68",
   "metadata": {},
   "source": [
    "Precision   =  True Positive / True Positive+False Positive\n",
    "\n",
    "Recall  =   True Positive  /  True Positive+False Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47659315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(\"Precision:\", precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68c8f555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Recall:\", recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd0a8be",
   "metadata": {},
   "source": [
    "F1 Score\n",
    "\n",
    "The F1 score is a way to balance precision and recall.\n",
    "\n",
    "F1 Score = (2 * recall * precision) / recall + precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2693c3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faf7099",
   "metadata": {},
   "source": [
    "CONFUSION MATRIX\n",
    "\n",
    "                     Positive(1)         Negative(0)\n",
    "\n",
    "                            TP                  FP\n",
    "                           FN                  TN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3f17ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc999acd",
   "metadata": {},
   "source": [
    "REGRESSION MATRICS  :- ( Find Relation between Independent and Dependent Variable)\n",
    "\n",
    "MEAN ABSOLUTE ERROR (MAE)\n",
    "\n",
    "MEAN SQUARED ERROR  (MSE)\n",
    "\n",
    "R-SQUARED   (Coefficient of Determination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeb118d",
   "metadata": {},
   "source": [
    "Mean Absolute Error(MAE)\n",
    "\n",
    "Metric used to measure the average absolute differances between predicted and actual values in a dataset.\n",
    "\n",
    "MAE= (1/n)n∑i=1 ∣ yi − y^i ∣\n",
    "\n",
    "y(i) = actual value\n",
    "y^(i) = predicted value\n",
    "n = number of observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be857df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print(\"MAE:\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0596942",
   "metadata": {},
   "source": [
    "Mean Squared Error (MSE)\n",
    "\n",
    "Measures the average of the squared differences between actual and predicted values. Penalizes large errors more than MAE.\n",
    "\n",
    "MAE= (1/n)n∑i=1 ( yi − y^i )²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71122160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "print(\"MSE:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ac57c0",
   "metadata": {},
   "source": [
    "R-Squared  (R² Score)\n",
    "\n",
    "Indicates how well the model explains the variance in the target variable.\n",
    "\n",
    "R² = 1 → Perfect model\n",
    "\n",
    "R² = 0 → Model predicts mean only\n",
    "\n",
    "R² < 0 → Worse than predicting mean\n",
    "\n",
    "Formula :-\n",
    "\n",
    "        R² = 1 - [∑( yi − y^i )² / ∑( yi − y`i )² ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81f33c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score: 0.9486081370449679\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "print(\"R2 Score:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a488ba40",
   "metadata": {},
   "source": [
    "Root Mean Squared Error (RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7b07af",
   "metadata": {},
   "source": [
    "Square root of MSE. Helps bring the error metric back to the same unit as the target variable.\n",
    "\n",
    "RMSE=  sqrt.MSE\n",
    "​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49fbf51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.6123724356957945\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "print(\"RMSE:\", rmse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
