{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7039f77",
   "metadata": {},
   "source": [
    "What is Supervised Learning (Regression)?\n",
    "\n",
    "Regression is a supervised learning technique used to predict continuous numerical values (e.g., price, temperature, salary, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410b360c",
   "metadata": {},
   "source": [
    "Types of Regression Algorithms in Supervised Learning\n",
    "\n",
    "| Algorithm                                             | Type                   | Use Cases                 | Example                                   |\n",
    "| ----------------------------------------------------- | ---------------------- | ------------------------- | ----------------------------------------- |\n",
    "| **1. Linear Regression**                              | Linear                 | Predicting prices, scores | House price prediction                    |\n",
    "| **2. Polynomial Regression**                          | Non-linear             | Curved data trends        | Growth curve modeling                     |\n",
    "| **3. Ridge Regression**                               | Regularized linear     | Multicollinearity         | Predicting rent prices                    |\n",
    "| **4. Lasso Regression**                               | Regularized linear     | Feature selection         | Stock price prediction                    |\n",
    "| **5. Elastic Net Regression**                         | Combo of Lasso + Ridge | Balanced regularization   | Risk analysis                             |\n",
    "| **6. Decision Tree Regressor**                        | Non-linear             | Rule-based regression     | Car price estimation                      |\n",
    "| **7. Random Forest Regressor**                        | Ensemble               | High performance          | Salary prediction                         |\n",
    "| **8. Gradient Boosting Regressor**                    | Ensemble               | Accurate prediction tasks | Energy load forecasting                   |\n",
    "| **9. SVR (Support Vector Regression)**                | Margin-based           | Complex, small datasets   | Time series forecasting                   |\n",
    "| **10. MLP Regressor (Neural Network)**                | Deep learning          | Complex non-linear        | Predicting house price with many features |\n",
    "| **11. HGB Regressor (HistGradientBoostingRegressor)** | Scalable boosting      | Large data regression     | Click-through rate prediction             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd570c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37.2305456] -1.052880660116693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\nmodel.coef_       # This gives Œ≤‚ÇÅ ‚Äî Coefficient (Slope of the Line)\\nmodel.intercept_  # This gives Œ≤‚ÇÄ ‚Äî the intercept (Where the line croses the y-axis)\\n- y: Predicted output\\n- x: Input feature\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#- Linear Regression :-\n",
    "'''\n",
    "Use: It‚Äôs used for predicting a continuous target using a linear relationship between input features and output.\n",
    "\n",
    "How it works: Models relationship using a straight line\n",
    "'''\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression        #- This generates a synthetic regression dataset.- Helpful for testing and learning without using real-world data.\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10)\n",
    "''' \n",
    "- n_samples=100: Creates 100 data points.\n",
    "- n_features=1: Each point has one feature (i.e., one input variable).\n",
    "- noise=10: Adds some randomness to simulate real-world data, making the regression line imperfect.\n",
    "'''\n",
    "model = LinearRegression()      #- It will fit a line of the form y = m¬∑x + b to the data.\n",
    "model.fit(X, y)                 #- Internally, it computes the best-fitting line using least squares‚Äîminimizing the error between predicted and actual values.\n",
    "print(model.coef_, model.intercept_)\n",
    "\n",
    "''' \n",
    "model.coef_       # This gives Œ≤‚ÇÅ ‚Äî Coefficient (Slope of the Line)\n",
    "model.intercept_  # This gives Œ≤‚ÇÄ ‚Äî the intercept (Where the line croses the y-axis)\n",
    "- y: Predicted output\n",
    "- x: Input feature\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c9db66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40.] 10.000000000000028\n"
     ]
    }
   ],
   "source": [
    "#- Example :-\n",
    "\n",
    "X = [[1], [2], [3], [4], [5]]\n",
    "y = [50, 90, 130, 170, 210]\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "print(model.coef_, model.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d39f71",
   "metadata": {},
   "source": [
    "üîß Use Cases of Linear Regression\n",
    "Real-world applications where this model shines:\n",
    "- Predicting house prices based on size, location, etc.\n",
    "- Forecasting sales from advertising spend.\n",
    "- Estimating salary based on years of experience.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e47ed0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 50.  90. 130. 170. 210.]\n"
     ]
    }
   ],
   "source": [
    "#Polynomial Regression (which is a powerful way to model complex relationships between input and output)\n",
    "\n",
    "''' \n",
    "Use: Non-linear trend\n",
    "\n",
    "How it works: Adds polynomial features to linear regression\n",
    "'''\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures    #- This brings in a tool to convert original features into polynomial features.\n",
    "from sklearn.linear_model import LinearRegression       #- Even though it's ‚Äúlinear,‚Äù it's used on polynomial-transformed data to fit curves\n",
    "from sklearn.pipeline import make_pipeline              #- Allows chaining multiple steps in one go: preprocessing + model training.This simplifies code and ensures smooth data flow.\n",
    "\n",
    "\n",
    "model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
    "model.fit(X, y)\n",
    "print(model.predict(X[:5]))     #- - The prediction output will be the estimated values of y for the first five input rows of X.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c28ec12",
   "metadata": {},
   "source": [
    "Use Cases of Polynomial Regression :-\n",
    "\n",
    "| üìà Finance | Modeling profit growth or compound interest curves | \n",
    "| üè• Healthcare | Predicting patient recovery trends over time | \n",
    "| üå°Ô∏è Weather | Forecasting temperature with seasonal effects | \n",
    "| üöó Engineering | Vehicle performance modeling based on torque, speed, etc. | \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27160653",
   "metadata": {},
   "source": [
    "RIDGE REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756c34b9",
   "metadata": {},
   "source": [
    "What Is Ridge Regression?\n",
    "\n",
    "Ridge Regression is a type of linear regression that adds L2 regularization to the model. Regularization helps prevent overfitting by penalizing large coefficient values.\n",
    "\n",
    "Why use it?\n",
    "\n",
    "- Helps when there‚Äôs multicollinearity (i.e., features are highly correlated).\n",
    "- Works well when there are more features than observations or noisy data.\n",
    "- Keeps model complexity in check by shrinking coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "976ffe35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9917355371900827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\n- Returns R¬≤ score, which tells you how well your model explains the variability of the output.\\n- Score of 1.0 = perfect fit, 0.0 = no better than mean, negative = worse than guessing.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#- Ridge Regression\n",
    "\n",
    "''' \n",
    "Use: Ridge Regression improves linear regression when predictors are correlated (multicollinearity).\n",
    "\n",
    "How it works: Adds L2 penalty to linear regression (or) Adds an L2 penalty (sum of squared coefficients) to shrink coefficients and stabilize the model.\n",
    "'''\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model = Ridge(alpha=1.0)            # Step 1: Create Ridge Regression model\n",
    "''' \n",
    "- alpha controls the strength of regularization.\n",
    "- Higher alpha means more penalty ‚ûú simpler model.\n",
    "- Default is 1.0, but tuning it is key to good performance.\n",
    "'''\n",
    "model.fit(X, y)                     # Step 2: Train the model on features X and target y\n",
    "''' \n",
    "- X is the input features (like age, income, etc.)\n",
    "- y is the target variable (like house price, exam score, etc.)\n",
    "- The model learns coefficients that best fit the data\n",
    "'''\n",
    "print(model.score(X, y))            # Step 3: Evaluate the model with R^2 score\n",
    "''' \n",
    "- Returns R¬≤ score, which tells you how well your model explains the variability of the output.\n",
    "- Score of 1.0 = perfect fit, 0.0 = no better than mean, negative = worse than guessing.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0edbdb",
   "metadata": {},
   "source": [
    "Example :-\n",
    "\n",
    "Predict student exam scores based on:\n",
    "- Number of hours studied\n",
    "- Sleep hours\n",
    "- Class attendance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "510e1494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9097602228633961\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "X = np.array([[5, 8, 90],\n",
    "              [7, 6, 80],\n",
    "              [4, 9, 95]])   # Features: [study_hours, sleep_hours, attendance]\n",
    "y = np.array([85, 87, 82])   # Target: exam scores\n",
    "\n",
    "model = Ridge(alpha=0.5)\n",
    "''' \n",
    "üõ†Ô∏è model = Ridge(alpha=0.5)\n",
    "You're creating a Ridge Regression model from scikit-learn, and you're setting the regularization strength using the alpha parameter.\n",
    "üîç So what does alpha=0.5 do?\n",
    "- alpha controls how much penalty the model applies to large coefficient values.\n",
    "- Lower alpha (like 0.5) means less regularization, so the model is closer to standard linear regression.\n",
    "- Higher alpha (like 5.0 or 10.0) means more regularization, which shrinks the coefficients more and may help reduce overfitting\n",
    "\n",
    "'''\n",
    "model.fit(X, y)\n",
    "print(model.score(X, y))     # Output might be something like 0.92 (very good fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6f7fd5",
   "metadata": {},
   "source": [
    "üöÄ When to Use Ridge Over Regular Linear Regression?\n",
    "- Your model is overfitting\n",
    "- Your dataset has many correlated features\n",
    "- You want more stable and reliable predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e05940d",
   "metadata": {},
   "source": [
    "LASSO REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e707bcd1",
   "metadata": {},
   "source": [
    "üìò What Is Lasso Regression?\n",
    "\n",
    "Lasso Regression (Least Absolute Shrinkage and Selection Operator) is a linear model that includes L1 regularization. It's popular for both prediction and automatic feature selection.\n",
    "\n",
    "üß™ Definition\n",
    "\n",
    "- Adds a penalty based on the absolute value of coefficients.\n",
    "- This penalty can shrink some coefficients to zero, essentially removing unimportant features.\n",
    "- Great when you have many features, but only a few truly matter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f8d4793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.         -0.         -0.31171429]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\n- Displays the model‚Äôs learned coefficients.\\n- Any zero values mean those features were deemed unimportant and removed by Lasso.\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lasso regression\n",
    "''' \n",
    "Use: Feature selection\n",
    "\n",
    "How it works: Adds L1 penalty (shrinks coefficients to zero)\n",
    "'''\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "model = Lasso(alpha=0.1)\n",
    "'''\n",
    "- Initializes the model with regularization strength alpha = 0.1\n",
    "- Smaller alpha ‚Üí less penalty, more freedom to keep features\n",
    "- Larger alpha ‚Üí stronger penalty, more features dropped\n",
    "'''\n",
    "model.fit(X, y)\n",
    "''' \n",
    "- Trains the model to learn the relationship between:\n",
    "- X: features/input variables (e.g. age, income, etc.)\n",
    "- y: target/output variable (e.g. price, score, etc.)\n",
    "\n",
    "'''\n",
    "print(model.coef_)\n",
    "''' \n",
    "- Displays the model‚Äôs learned coefficients.\n",
    "- Any zero values mean those features were deemed unimportant and removed by Lasso.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b083a5",
   "metadata": {},
   "source": [
    "Example :-\n",
    "\n",
    "Predicting bike rental demand based on:\n",
    "- Temperature\n",
    "- Humidity\n",
    "- Wind speed\n",
    "- Holiday indicator\n",
    "- Day of the week\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9625f4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.    -1.997 -0.    -0.    -0.   ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "X = np.array([[25, 60, 12, 1, 2],\n",
    "              [30, 50, 8, 0, 5],\n",
    "              [22, 70, 10, 0, 6]])   # Features: [temp, humidity, wind, holiday, day]\n",
    "y = np.array([120, 140, 100])        # Target: bike rentals\n",
    "\n",
    "model = Lasso(alpha=0.2)\n",
    "model.fit(X, y)\n",
    "\n",
    "print(model.coef_)  # Some values may be 0 ‚Üí features auto-removed by Lasso\n",
    "\n",
    "#It means humidity and holiday aren‚Äôt adding useful info ‚Äî Lasso zeroed them out "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f1cab5",
   "metadata": {},
   "source": [
    "Why Use Lasso?\n",
    "\n",
    "- ‚úÖ Automatic feature selection (simpler, cleaner models)\n",
    "- ‚úÖ Prevents overfitting\n",
    "- ‚úÖ Handles high-dimensional datasets\n",
    "- ‚úÖ Useful when many features are irrelevant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93935eb",
   "metadata": {},
   "source": [
    "ELASTIC NET REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1ad044",
   "metadata": {},
   "source": [
    "üß† What Is Elastic Net Regression?\n",
    "\n",
    "Elastic Net Regression is a linear regression model enhanced with regularization techniques. It blends:\n",
    "- L1 penalty from Lasso Regression: promotes sparsity by shrinking some coefficients to zero (feature selection)\n",
    "- L2 penalty from Ridge Regression: discourages large coefficients (adds stability when predictors are correlated)\n",
    "Together, Elastic Net gives you the best of both worlds: feature selection and robustness against multicollinearity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e4aead",
   "metadata": {},
   "source": [
    "üîß How It Works\n",
    "\n",
    "Elastic Net minimizes the following cost function:\n",
    "\n",
    "\\text{Loss} = \\text{RSS} + \\alpha \\left( \\text{l1\\_ratio} \\cdot ||\\beta||_1 + (1 - \\text{l1\\_ratio}) \\cdot ||\\beta||_2^2 \\right)\n",
    "\n",
    "Where:\n",
    "- \\text{RSS} = Residual sum of squares (ordinary loss)\n",
    "- \\alpha = overall regularization strength\n",
    "- \\text{l1\\_ratio} controls how much L1 vs L2 to apply\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefe74bb",
   "metadata": {},
   "source": [
    "üéØ Use Cases\n",
    "\n",
    "Elastic Net is especially useful when:\n",
    "- There are many features, some of which are highly correlated\n",
    "- You need feature selection but Lasso drops too many or Ridge keeps too many\n",
    "- Data is noisy or high-dimensional (like in genomics, finance, or image recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0e86739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999997213768693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\nLet‚Äôs say you‚Äôre predicting house prices with 100 features, some of which overlap‚Äîlike area in square feet and number of rooms. A plain linear regression may struggle with collinearity. Elastic Net:\\n- Shrinks unimportant features (L2)\\n- Eliminates redundant ones (L1)\\n- Boosts prediction accuracy and model interpretability\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Create model with 50% L1 and 50% L2 penalty\n",
    "model = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "\n",
    "# Fit model to training data\n",
    "model.fit(X, y)\n",
    "\n",
    "# Evaluate performance\n",
    "print(model.score(X, y))  # R¬≤ score on training set\n",
    "\n",
    "''' \n",
    "Let‚Äôs say you‚Äôre predicting house prices with 100 features, some of which overlap‚Äîlike area in square feet and number of rooms. A plain linear regression may struggle with collinearity. Elastic Net:\n",
    "- Shrinks unimportant features (L2)\n",
    "- Eliminates redundant ones (L1)\n",
    "- Boosts prediction accuracy and model interpretability\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719ced6f",
   "metadata": {},
   "source": [
    "DECISION TREE REGRESSOR\n",
    "\n",
    "Definition :- \n",
    "\n",
    "A Decision Tree Regressor is a supervised machine learning algorithm that predicts a continuous value by learning rules from data. It builds a tree structure that splits data into smaller subsets based on feature thresholds, ending in leaf nodes that hold predictions.\n",
    "\n",
    "How It Works\n",
    "\n",
    "- Start at the Root Node: The model looks at all features and finds the split that minimizes prediction error (usually using Mean Squared Error).\n",
    "- Recursive Splitting: It continues splitting each subset by selecting features and thresholds until:\n",
    "- A stopping condition is met (e.g. maximum depth, minimum samples per leaf).\n",
    "- No further improvement is possible.\n",
    "- Leaf Nodes: Each leaf holds a constant prediction value‚Äîthe mean of the target values in that subset.\n",
    "\n",
    "Use Cases\n",
    "\n",
    "Decision Trees shine when:\n",
    "- You want easy interpretability.\n",
    "- Your data has non-linear relationships.\n",
    "- You have mixed-type features (numerical and categorical).\n",
    "- There‚Äôs no need to generalize beyond training data (risk of overfitting).\n",
    "Common in:\n",
    "- Forecasting stock prices\n",
    "- Predicting medical costs\n",
    "- Estimating house prices\n",
    "- Customer behavior modeling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc182ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120. 140. 100.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Initialize model with default parameters\n",
    "model = DecisionTreeRegressor()\n",
    "\n",
    "# Train (fit) model to feature matrix X and target y\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict values for first 5 instances in X\n",
    "print(model.predict(X[:5]))\n",
    "\n",
    "''' \n",
    "Example :-\n",
    "Let‚Äôs say you‚Äôre predicting salaries based on years of experience, education level, and job role.\n",
    "- The tree first checks ‚ÄúIs experience > 5 years?‚Äù\n",
    "- Then maybe ‚ÄúIs education = Master‚Äôs?‚Äù\n",
    "- At each step, it narrows the decision to groups with similar salary ranges.\n",
    "In the end, each leaf holds the average salary value for a sub-group of employees that match a certain path of rules.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef144117",
   "metadata": {},
   "source": [
    "RANDOM FOREST REGRESSOR\n",
    "\n",
    "What Is Random Forest Regression?\n",
    "\n",
    "A Random Forest Regressor is an ensemble model that builds multiple decision trees during training and predicts by averaging their outputs. It combines the simplicity of decision trees with the accuracy boost of bagging (bootstrap aggregating).\n",
    "\n",
    "How It Works\n",
    "\n",
    "- Bootstrapping: The model draws multiple random samples (with replacement) from the original dataset.\n",
    "- Tree Building: For each sample, a decision tree is built using random subsets of features.\n",
    "- Prediction Averaging: For regression, the final prediction is the average of predictions from all trees.\n",
    "This randomness reduces overfitting and improves generalization\n",
    "\n",
    " Use Cases\n",
    " \n",
    "Random Forest is ideal when:\n",
    "- You need strong prediction accuracy out-of-the-box.\n",
    "- Your data has lots of features, possibly with non-linear relationships.\n",
    "- You're dealing with tabular datasets, either clean or noisy.\n",
    "Popular in:\n",
    "- Environmental modeling (e.g. rainfall prediction)\n",
    "- Healthcare (e.g. risk scoring)\n",
    "- Real estate price estimates\n",
    "- Fraud detection in financ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05819e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[119.2 129.8 112.4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\nSuppose you‚Äôre predicting crop yield based on soil quality, rainfall, temperature, and fertilizer type. One decision tree might overfit to rainfall patterns. But with 100 slightly different trees, Random Forest will:\\n- Smooth out noisy decisions\\n- Leverage diverse perspectives from all trees\\n- Result in a more stable and accurate prediction\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Create a random forest regressor model\n",
    "model = RandomForestRegressor()     #initializes the model using default settings (100 trees).\n",
    "\n",
    "# Fit model to training data\n",
    "model.fit(X, y)                     #trains each tree on different slices of data\n",
    "\n",
    "# Predict on first five instances\n",
    "print(model.predict(X[:5]))         #outputs an averaged prediction across trees for each input\n",
    "\n",
    "''' \n",
    "Suppose you‚Äôre predicting crop yield based on soil quality, rainfall, temperature, and fertilizer type. One decision tree might overfit to rainfall patterns. But with 100 slightly different trees, Random Forest will:\n",
    "- Smooth out noisy decisions\n",
    "- Leverage diverse perspectives from all trees\n",
    "- Result in a more stable and accurate prediction\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1540fd",
   "metadata": {},
   "source": [
    "GRADIENT BOOSTING REGRESSOR\n",
    "\n",
    "What Is Gradient Boosting Regressor?\n",
    "\n",
    "Gradient Boosting is a powerful ensemble technique that builds a strong predictor by combining many weak learners‚Äîusually shallow decision trees. Each tree corrects the errors of the previous one, producing highly accurate models, especially for structured/tabular data.\n",
    "\n",
    "How It Works\n",
    "\n",
    "The core idea: sequential learning with gradient descent optimization.\n",
    "- Start with an initial prediction (usually the mean of targets).\n",
    "- Calculate the residuals (errors from previous prediction).\n",
    "- Fit a tree to these residuals.\n",
    "- Add the new tree's predictions (scaled by a learning rate) to the previous model.\n",
    "- Repeat for a set number of iterations (trees).\n",
    "This step-by-step error correction allows the model to focus on what previous trees got wrong.\n",
    "\n",
    "Use Cases\n",
    "\n",
    "Use Gradient Boosting when:\n",
    "- You need high predictive accuracy on structured/tabular data.\n",
    "- Relationships are non-linear and complex.\n",
    "- You‚Äôre dealing with heterogeneous feature sets.\n",
    "Common applications:\n",
    "- Credit scoring and risk modeling üí≥\n",
    "- Sales forecasting üìà\n",
    "- Disease progression modeling üß¨\n",
    "- Energy consumption predictions \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "915c40cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120.         139.99946877 100.00053123]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Create the model with default settings\n",
    "model = GradientBoostingRegressor()\n",
    "''' \n",
    "- GradientBoostingRegressor() uses parameters like:\n",
    "- n_estimators: number of trees\n",
    "- learning_rate: how much each tree contributes\n",
    "- max_depth: depth of each tree\n",
    "\n",
    "'''\n",
    "\n",
    "# Fit to training data\n",
    "model.fit(X, y)             #builds the sequence of trees to minimize error.\n",
    "\n",
    "# Predict for the first 5 samples\n",
    "print(model.predict(X[:5]))     #produces predictions for the first five samples‚Äîaveraged across all trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbcca16",
   "metadata": {},
   "source": [
    "SVR (SUPPORT VECTOR REGRESSION)\n",
    "\n",
    "Definition :- \n",
    "\n",
    "SVR (Support Vector Regression) is a type of Support Vector Machine used for predicting continuous values. It doesn‚Äôt try to perfectly fit the data‚Äîinstead, it finds a function that deviates from actual values by no more than a certain margin (epsilon), while keeping the model as flat (simple) as possible.\n",
    "It prioritizes robustness and generalization, especially for small to medium-sized datasets.\n",
    "\n",
    "How It Works\n",
    "\n",
    "- SVR defines a margin of tolerance (epsilon) where errors within this zone are ignored.\n",
    "- It only considers data points outside this margin for model correction‚Äîthese are the \"support vectors.\"\n",
    "- A kernel function (e.g., rbf) transforms data into higher dimensions, enabling non-linear regression.\n",
    "- The model learns weights that define the regression hyperplane while keeping most coefficients at zero‚Äîmaking it sparse and efficient\n",
    "\n",
    "When to Use SVR\n",
    "\n",
    "SVR is ideal when:\n",
    "- Your dataset is not too large (computationally expensive for big data).\n",
    "- You need a non-linear model with flexibility (especially with kernels).\n",
    "- Accuracy matters, but you want to tolerate small prediction errors.\n",
    "- Data might be noisy, and you want to avoid overfitting.\n",
    "Applications include:\n",
    "- Time series forecasting üìä\n",
    "- Stock price prediction üíπ\n",
    "- Environmental modeling üåßÔ∏è\n",
    "- Engineering simulations \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30444bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120.00000003 120.18247461 119.8337276 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\nYou can tweak C (penalty for errors), epsilon (margin width), and gamma (kernel coefficient) for better control and performance\\n\\nExample\\n\\nSay you‚Äôre predicting battery health based on usage patterns, temperature, and charge cycles:\\n- SVR will build a regression function that captures the trend while ignoring minor deviations.\\n- Only outlier samples (where predictions exceed the epsilon margin) influence model updates.\\n- This creates a balanced model that doesn‚Äôt overreact to noisy data.\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# Create SVR model with Radial Basis Function kernel\n",
    "model = SVR(kernel='rbf')       #chooses a non-linear kernel to handle complex patterns.\n",
    "\n",
    "# Fit model to training data\n",
    "model.fit(X, y)                 #learns the regression function using support vectors.\n",
    "\n",
    "# Predict on first 5 inputs\n",
    "print(model.predict(X[:5]))     #gives continuous predictions for the first 5 samples\n",
    "\n",
    "''' \n",
    "You can tweak C (penalty for errors), epsilon (margin width), and gamma (kernel coefficient) for better control and performance\n",
    "\n",
    "Example\n",
    "\n",
    "Say you‚Äôre predicting battery health based on usage patterns, temperature, and charge cycles:\n",
    "- SVR will build a regression function that captures the trend while ignoring minor deviations.\n",
    "- Only outlier samples (where predictions exceed the epsilon margin) influence model updates.\n",
    "- This creates a balanced model that doesn‚Äôt overreact to noisy data.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a509a4bf",
   "metadata": {},
   "source": [
    "MLP REGRESSOR (NUERAL NETWORK)\n",
    "\n",
    "What Is MLP Regressor?\n",
    "\n",
    "An MLP Regressor is a feedforward neural network designed for regression tasks. It consists of fully connected layers of neurons that learn complex nonlinear mappings from inputs to outputs using backpropagation and gradient descent.\n",
    "\n",
    "How It Works\n",
    "\n",
    "- Architecture:\n",
    "- Input layer ‚Üí Hidden layers ‚Üí Output layer\n",
    "- Each neuron applies a weighted sum of inputs, adds bias, and passes through an activation function (typically ReLU or tanh).\n",
    "- Training:\n",
    "- The model starts with random weights.\n",
    "- It predicts output values, compares them to actual targets using a loss function (usually MSE).\n",
    "- Backpropagation adjusts weights to minimize error.\n",
    "- This repeats over multiple iterations (max_iter) until convergence.\n",
    "\n",
    "When to Use MLP Regressor\n",
    "\n",
    "MLP Regressor works best when:\n",
    "- You have nonlinear relationships that simpler models (like trees or linear regressors) can‚Äôt capture.\n",
    "- The dataset size is moderate (too large, and training can be slow).\n",
    "- You‚Äôre comfortable tuning parameters like layer sizes, learning rate, and activation functions.\n",
    "It‚Äôs often used for:\n",
    "- Predicting housing or vehicle prices üè°üöó\n",
    "- Modeling climate or energy consumption ‚òÄÔ∏è‚ö°\n",
    "- Forecasting demand or user engagement üìà\n",
    "- Any regression task requiring deep, abstract feature learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd4f879e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[119.93687987 140.14295046  99.94516437]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\nExample\\nImagine predicting air pollution levels based on traffic, weather, time, and industrial activity. Linear models may miss the interactions. MLP can:\\n- Encode nonlinear effects (e.g., traffic and humidity combined may amplify pollution)\\n- Learn hidden patterns through its deep layers\\n- Yield more accurate predictions where simpler models fail\\n\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Create a neural network model with default architecture\n",
    "model = MLPRegressor(max_iter=1000)\n",
    "''' \n",
    "- MLPRegressor() builds the neural network. \n",
    "You can customize layers using hidden_layer_sizes=(100,), \n",
    "activation with activation='relu', and solver with solver='adam'.\n",
    "\n",
    "'''\n",
    "\n",
    "# Train it on your feature matrix X and target variable y\n",
    "model.fit(X, y)     #trains the model using gradient descent.\n",
    "\n",
    "# Predict outputs for first 5 samples\n",
    "print(model.predict(X[:5]))     #outputs predicted values based on learned weights.\n",
    "\n",
    "''' \n",
    "Example\n",
    "Imagine predicting air pollution levels based on traffic, weather, time, and industrial activity. Linear models may miss the interactions. MLP can:\n",
    "- Encode nonlinear effects (e.g., traffic and humidity combined may amplify pollution)\n",
    "- Learn hidden patterns through its deep layers\n",
    "- Yield more accurate predictions where simpler models fail\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9eef41",
   "metadata": {},
   "source": [
    "HISTOGRAM-BASED GRADIENT BOOSTING REGRESSOR(HGB)\n",
    "\n",
    "What Is HGB Regressor?\n",
    "\n",
    "The HistGradientBoostingRegressor is a high-speed, scalable version of gradient boosting, designed for large datasets. It uses histogram-based binning of numerical features to speed up training and reduce memory usage‚Äîmaking it perfect for real-world tabular data.\n",
    "It‚Äôs part of scikit-learn‚Äôs experimental ensemble module, inspired by the LightGBM framework.\n",
    "\n",
    "How It Works\n",
    "\n",
    "HGB speeds things up using these tricks:\n",
    "- Feature Binning:\n",
    "- Instead of using raw feature values, it groups them into discrete bins.\n",
    "- This reduces the number of possible splits in trees.\n",
    "- Gradient Boosting Strategy:\n",
    "- Builds trees sequentially, each learning from the errors (residuals) of the previous ones.\n",
    "- Optimizes predictions using gradient descent.\n",
    "By binning features, the model can handle datasets with millions of rows efficiently\n",
    "\n",
    "When to Use It\n",
    "\n",
    "Use HGB when:\n",
    "- Your dataset is large and high-dimensional.\n",
    "- You need both accuracy and speed.\n",
    "- You‚Äôre solving regression on structured/tabular data.\n",
    "It shines in:\n",
    "- E-commerce: price/demand predictions üí∞\n",
    "- Energy modeling: load forecasting üîã\n",
    "- Web analytics: engagement or conversion forecasting \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e8a377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120. 120. 120.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\nYou can tune:\\n- max_iter (number of trees)\\n- learning_rate\\n- max_depth\\n- max_bins (for histogram resolution)\\n\\nExample\\n\\nSay you‚Äôre modeling real estate prices across an entire country, with millions of records. Traditional models may struggle. But HGB:\\n- Bins features like square footage, location score, and age of property\\n- Builds fast, accurate models using fewer calculations\\n- Keeps memory usage low, even as data scales\\n\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "# Initialize the fast gradient boosting model\n",
    "model = HistGradientBoostingRegressor()     #uses efficient feature binning\n",
    "\n",
    "# Fit to training data\n",
    "model.fit(X, y)                     #builds and trains gradient-boosted trees.\n",
    "\n",
    "# Predict outputs for first 5 samples\n",
    "print(model.predict(X[:5]))         #makes fast predictions using the ensemble\n",
    "\n",
    "''' \n",
    "You can tune:\n",
    "- max_iter (number of trees)\n",
    "- learning_rate\n",
    "- max_depth\n",
    "- max_bins (for histogram resolution)\n",
    "\n",
    "Example\n",
    "\n",
    "Say you are modeling real estate prices across an entire country, with millions of records. Traditional models may struggle. But HGB:\n",
    "- Bins features like square footage, location score, and age of property\n",
    "- Builds fast, accurate models using fewer calculations\n",
    "- Keeps memory usage low, even as data scales\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e843c1aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
